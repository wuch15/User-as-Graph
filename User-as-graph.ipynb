{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_train.zip\n",
    "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_dev.zip    \n",
    "!mkdir train\n",
    "!mkdir val\n",
    "!unzip MINDlarge_train.zip -d ./train\n",
    "!unzip MINDlarge_dev.zip -d ./val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import cholesky\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model \n",
    "from keras import activations, constraints, initializers, regularizers\n",
    "from keras import backend as K \n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from sklearn.metrics import *\n",
    "from keras.optimizers import *\n",
    "import keras \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train/behaviors.tsv')as f:\n",
    "    trainuser=f.readlines()\n",
    "\n",
    "with open('val/behaviors.tsv') as f:\n",
    "    valuser=f.readlines()\n",
    "\n",
    "with open('train/news.tsv')as f:\n",
    "    data=f.readlines()\n",
    "\n",
    "with open('val/news.tsv')as f:\n",
    "    data+=f.readlines()\n",
    "\n",
    "with open('train/entity_embedding.vec')as f:\n",
    "    entity_emb=f.readlines()\n",
    "    \n",
    "with open('val/entity_embedding.vec')as f:\n",
    "    entity_emb+=f.readlines()\n",
    "\n",
    "entity_emb_dict={}\n",
    "for i in entity_emb:\n",
    "    entity_emb_dict[i.strip().split('\\t')[0]]=[float(x) for x in i.strip().split('\\t')[1:]]\n",
    "\n",
    "entityidlist={'NULL':0}\n",
    "entity_emb_table=[[0.]*100]\n",
    "cnt=0.\n",
    "for i in news:\n",
    "    for j in news[i][3]:\n",
    "        if j not in entityidlist:\n",
    "            entityidlist[j]=len(entityidlist)\n",
    "            entity_emb_table.append(entity_emb_dict.get(j,np.random.uniform(-0.03,0.03,(100,))))\n",
    "entity_emb_table=np.array(entity_emb_table,dtype=np.float32)\n",
    "print(entity_emb_table.shape)\n",
    "\n",
    "\n",
    "news={}\n",
    "category={'NULL':0}\n",
    "subcategory={'NULL':0}\n",
    "newsnumber=0\n",
    "for i in data:\n",
    "    line=i.strip('\\n').split('\\t')\n",
    "    if line[0] not in news:\n",
    "        news[line[0]]=[line[1],line[2],word_tokenize(line[3].lower()),[x[\"WikidataId\"] for x in json.loads(line[6])]]\n",
    "    if line[1] not in category:\n",
    "        category[line[1]]=len(category)\n",
    "    if line[2] not in subcategory:\n",
    "        subcategory[line[2]]=len(subcategory)\n",
    "    newsnumber+=1\n",
    "    if newsnumber%1000==0:\n",
    "        print(newsnumber)\n",
    "\n",
    "newsindex={'NULL':0}\n",
    "for i in news:\n",
    "    newsindex[i]=len(newsindex)\n",
    "\n",
    "word_dict={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            word_dict[j][1]+=1\n",
    "        else:\n",
    "            word_dict[j]=[len(word_dict),1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "embdict={}\n",
    "with open('glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        line=f.readline()\n",
    "        if len(line)==0:\n",
    "            break\n",
    "        line = line.split()\n",
    "        word=line[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            vec=[float(x) for x in line[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=vec\n",
    "\n",
    "\n",
    "emb_mat=[0]*len(word_dict)\n",
    "in_dict_emb=[]\n",
    "for i in embdict.keys():\n",
    "    emb_mat[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    in_dict_emb.append(emb_mat[word_dict[i][0]])\n",
    "in_dict_emb=np.array(in_dict_emb,dtype='float32')\n",
    "\n",
    "mu=np.mean(in_dict_emb, axis=0)\n",
    "Sigma=np.cov(in_dict_emb.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(emb_mat)):\n",
    "    if type(emb_mat[i])==int:\n",
    "        emb_mat[i]=np.reshape(norm, 300)\n",
    "emb_mat[0]=np.zeros(300,dtype='float32')\n",
    "emb_mat=np.array(emb_mat,dtype='float32')\n",
    "print(emb_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words=[[0]*30]\n",
    "\n",
    "for i in news:\n",
    "    words=[]\n",
    "    for word in news[i][2]:\n",
    "        if word in word_dict:\n",
    "            words.append(word_dict[word][0])\n",
    "    words=words[:30]\n",
    "    news_words.append(words+[0]*(30-len(words)))\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "\n",
    "news_entity=[[0]*5]\n",
    "\n",
    "for i in news:\n",
    "    entities=[]\n",
    "    for entity in news[i][3]:\n",
    "        if entity in entityidlist:\n",
    "            entities.append(entityidlist[entity])\n",
    "    entities=entities[:5]\n",
    "    entities=entities+[0]*(5-len(entities))\n",
    "    news_entity.append(entities)\n",
    "news_entity=np.array(news_entity,dtype='int32') \n",
    "\n",
    "news_topic=[0]\n",
    "for i in news:\n",
    "     \n",
    "    news_topic.append(category[news[i][0]])\n",
    "news_topic=np.array(news_topic,dtype='int32')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(array,ratio):\n",
    "    if ratio >len(array):\n",
    "        return random.sample(array*(ratio//len(array)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(array,ratio)\n",
    "    \n",
    "npratio=4\n",
    "train_candidate=[]    \n",
    "train_label=[]\n",
    "train_user_his=[]\n",
    "\n",
    "for user in trainuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x] for x in userline[3].split()][-50:]\n",
    "    pdoc=[newsindex[x.split('-')[0]] for x in userline[4].split() if x.split('-')[1]=='1']\n",
    "    ndoc=[newsindex[x.split('-')[0]] for x in userline[4].split() if x.split('-')[1]=='0']\n",
    "    \n",
    "    for doc in pdoc:\n",
    "        negd=newsample(ndoc,npratio)\n",
    "        negd.append(doc)\n",
    "        candidate_label=[0]*npratio+[1]\n",
    "        candidate_order=list(range(npratio+1))\n",
    "        random.shuffle(candidate_order)\n",
    "        candidate_shuffle=[]\n",
    "        candidate_label_shuffle=[]\n",
    "        for i in candidate_order:\n",
    "            candidate_shuffle.append(negd[i])\n",
    "            candidate_label_shuffle.append(candidate_label[i])\n",
    "        train_candidate.append(candidate_shuffle)\n",
    "        train_label.append(candidate_label_shuffle)\n",
    "        train_user_his.append(clickids+[0]*(50-len(clickids))) \n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_candidate=[] \n",
    "test_user_his=[]\n",
    "test_index=[]\n",
    "test_label=[]\n",
    "\n",
    "for user in valuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x] for x in userline[3].split()][-50:]\n",
    "    docs=[newsindex[x.split('-')[0]] for x in userline[4].split()]\n",
    "    index=[]\n",
    "    index.append(len(test_candidate))\n",
    "  \n",
    "    test_user_his.append(clickids+[0]*(50-len(clickids)))\n",
    "    for x in userline[4].split():\n",
    "        test_label.append(int(x.split('-')[1]))\n",
    "    for doc in docs:\n",
    "        test_candidate.append(doc)\n",
    "    index.append(len(test_candidate))\n",
    "    test_index.append(index)\n",
    "\n",
    "\n",
    "train_candidate=np.array(train_candidate,dtype='int32')\n",
    "train_label=np.array(train_label,dtype='int32')\n",
    "train_user_his=np.array(train_user_his,dtype='int32')\n",
    "\n",
    "test_candidate=np.array(test_candidate,dtype='int32') \n",
    "test_user_his=np.array(test_user_his,dtype='int32')\n",
    "test_label=np.array(test_label,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_random(batch_size):\n",
    "    idx = np.arange(len(train_label))\n",
    "    np.random.shuffle(idx)\n",
    "    y=train_label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    " \n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            \n",
    "            item_words = news_words[train_candidate[i]]\n",
    "            \n",
    "            item_topic=news_topic[train_candidate[i]]\n",
    "            item_entity=news_entity[train_candidate[i]]\n",
    "            \n",
    "            user_his=news_words[train_user_his[i]]\n",
    "            user_topic=news_topic[train_user_his[i]]\n",
    "            user_entity=news_entity[train_user_his[i]]\n",
    "            user_entity_feature=[]\n",
    "            all_A=[]\n",
    "            for s in range(len(i)):\n",
    "                Asize=len(train_user_his[i][s])+len(category)+50\n",
    "                newsA=np.zeros((Asize,Asize))\n",
    "                entityid_set={}\n",
    "                entityid_set_ids=[]\n",
    "                for el in range(len(user_entity[s])):\n",
    "                    for e in range(len(user_entity[s][el])):\n",
    "                        if user_entity[s][el][e] not in entityid_set:\n",
    "                            entityid_set[user_entity[s][el][e]]=len(entityid_set)\n",
    "                            entityid_set_ids.append(user_entity[s][el][e])\n",
    "                        if user_entity[s][el][e]!=0  and entityid_set[user_entity[s][el][e]]<50:\n",
    "                            newsA[el][len(train_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]]=1\n",
    "                            newsA[len(train_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]][el]=1\n",
    "                            newsA[len(train_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]][len(train_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]]=1\n",
    "                entityid_set_ids=entityid_set_ids[:50]\n",
    "                entityid_set_ids+=[0]*(50-len(entityid_set_ids))\n",
    "        \n",
    "                for m in range(len(train_user_his[i][s])):\n",
    "                    if train_user_his[i][s][m]!=0:\n",
    "                        newsA[m][m]=1\n",
    "                        if m>=1 and train_user_his[i][s][m-1]!=0:\n",
    "                            newsA[m][m-1]=1\n",
    "                        if m<len(train_user_his[i][s])-1 and train_user_his[i][s][m+1]!=0:\n",
    "                            newsA[m][m+1]=1\n",
    "                    if user_topic[s][m]!=0:\n",
    "                        newsA[m][len(train_user_his[i][s])+user_topic[s][m]]=1\n",
    "                        newsA[len(train_user_his[i][s])+user_topic[s][m]][m]=1\n",
    "                        newsA[len(train_user_his[i][s])+user_topic[s][m]][len(train_user_his[i][s])+user_topic[s][m]]=1\n",
    "                user_entity_feature.append(entityid_set_ids)\n",
    "                all_A.append(newsA)\n",
    "            user_entity_feature=np.array(user_entity_feature)                \n",
    "    \n",
    "    \n",
    "            all_A=np.array(all_A,dtype='float32')\n",
    "            \n",
    "            topics=np.array([list(range(len(category)))]*len(i),dtype='int32')\n",
    "                   \n",
    "            yield ([item_words,user_his,item_topic,item_entity,user_entity_feature,topics,all_A], [train_label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from the GAT repo\n",
    "class GraphAttention(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 F_,\n",
    "                 attn_heads=20,\n",
    "                 attn_heads_reduction='concat',  # {'concat', 'average'}\n",
    "                 dropout_rate=0.2,\n",
    "                 activation='relu',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 attn_kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 attn_kernel_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 attn_kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        if attn_heads_reduction not in {'concat', 'average'}:\n",
    "            raise ValueError('Possbile reduction methods: concat, average')\n",
    "\n",
    "        self.F_ = F_  # Number of output features (F' in the paper)\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  # Eq. 5 and 6 in the paper\n",
    "        self.dropout_rate = dropout_rate  # Internal dropout rate\n",
    "        self.activation = activations.get(activation)  # Eq. 4 in the paper\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n",
    "        self.supports_masking = False\n",
    "\n",
    "        # Populated by build()\n",
    "        self.kernels = []       # Layer kernels for attention heads\n",
    "        self.biases = []        # Layer biases for attention heads\n",
    "        self.attn_kernels = []  # Attention kernels for attention heads\n",
    "\n",
    "        if attn_heads_reduction == 'concat':\n",
    "            # Output will have shape (..., K * F')\n",
    "            self.output_dim = self.F_ * self.attn_heads\n",
    "        else:\n",
    "            # Output will have shape (..., F')\n",
    "            self.output_dim = self.F_\n",
    "\n",
    "        super(GraphAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        F = input_shape[0][-1]\n",
    "\n",
    "        # Initialize weights for each attention head\n",
    "        for head in range(self.attn_heads):\n",
    "            # Layer kernel\n",
    "            kernel = self.add_weight(shape=(F, self.F_),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     constraint=self.kernel_constraint,\n",
    "                                     name='kernel_{}'.format(head))\n",
    "            self.kernels.append(kernel)\n",
    "\n",
    "            # # Layer bias\n",
    "            if self.use_bias:\n",
    "                bias = self.add_weight(shape=(self.F_, ),\n",
    "                                       initializer=self.bias_initializer,\n",
    "                                       regularizer=self.bias_regularizer,\n",
    "                                       constraint=self.bias_constraint,\n",
    "                                       name='bias_{}'.format(head))\n",
    "                self.biases.append(bias)\n",
    "\n",
    "            # Attention kernels\n",
    "            attn_kernel_self = self.add_weight(shape=(self.F_, 1),\n",
    "                                               initializer=self.attn_kernel_initializer,\n",
    "                                               regularizer=self.attn_kernel_regularizer,\n",
    "                                               constraint=self.attn_kernel_constraint,\n",
    "                                               name='attn_kernel_self_{}'.format(head),)\n",
    "            attn_kernel_neighs = self.add_weight(shape=(self.F_, 1),\n",
    "                                                 initializer=self.attn_kernel_initializer,\n",
    "                                                 regularizer=self.attn_kernel_regularizer,\n",
    "                                                 constraint=self.attn_kernel_constraint,\n",
    "                                                 name='attn_kernel_neigh_{}'.format(head))\n",
    "            self.attn_kernels.append([attn_kernel_self, attn_kernel_neighs])\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs[0]  # Node features (N x F)\n",
    "        A = inputs[1]  # Adjacency matrix (N x N)\n",
    "\n",
    "        outputs = []\n",
    "        for head in range(self.attn_heads):\n",
    "            kernel = self.kernels[head]  # W in the paper (F x F')\n",
    "            attention_kernel = self.attn_kernels[head]  # Attention kernel a in the paper (2F' x 1)\n",
    "\n",
    "            # Compute inputs to attention network\n",
    "            features = K.dot(X, kernel)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            # Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]\n",
    "            attn_for_self = K.dot(features, attention_kernel[0])    # (N x 1), [a_1]^T [Wh_i]\n",
    "            attn_for_neighs = K.dot(features, attention_kernel[1])  # (N x 1), [a_2]^T [Wh_j]\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + K.permute_dimensions(attn_for_neighs,(0,2,1))  # (N x N) via broadcasting\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = LeakyReLU(alpha=0.2)(dense)\n",
    "\n",
    "            # Mask values before activation (Vaswani et al., 2017)\n",
    "            mask = -10e9 * (1.0 - A)\n",
    "            dense += mask\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = K.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = Dropout(self.dropout_rate)(dense)  # (N x N)\n",
    "            dropout_feat = Dropout(self.dropout_rate)(features)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = K.batch_dot(dropout_attn, dropout_feat,axes=[2,1])  # (N x F')\n",
    "            \n",
    "            if self.use_bias:\n",
    "                node_features = K.bias_add(node_features, self.biases[head])\n",
    "            \n",
    "            # Add output of attention head to final output\n",
    "            outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = K.concatenate(outputs)  # (N x KF')\n",
    "        else:\n",
    "            output = K.mean(K.stack(outputs), axis=0)  # N x F')\n",
    "        \n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "        return output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x): \n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x \n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3)) \n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A) \n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, dim=200,**kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.dim = dim\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        dim = self.dim\n",
    "        self.W1 = K.variable(self.init((input_shape[-1], dim)))\n",
    "        self.b1 = K.variable(self.init((dim,)))\n",
    "        self.q1 = K.variable(self.init((dim, 1)))\n",
    "         \n",
    "        self.trainable_weights = [self.W1, self.b1, self.q1]\n",
    "        super(AttLayer,self).build(input_shape)  \n",
    "\n",
    "    def call(self, inputs, **kwargs): \n",
    "        attention1 = K.tanh(K.dot(inputs, self.W1) + self.b1)\n",
    "        attention1 = K.dot(attention1, self.q1)\n",
    "        attention1 = K.squeeze(attention1, axis=2)\n",
    "        \n",
    "        attention = attention1\n",
    "        attention = K.exp(attention)\n",
    "        attention_weight = attention / (K.sum(attention, axis=-1, keepdims=True) + K.epsilon())\n",
    "\n",
    "        attention_weight = K.expand_dims(attention_weight)\n",
    "        weighted_input = inputs * attention_weight\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "keras.backend.clear_session() \n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "topic_input = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(word_dict), 300, weights=[emb_mat],trainable=True)\n",
    "\n",
    "topic_embedding_layer = Embedding(len(category),256,trainable=True)\n",
    "entity_embedding_layer = Embedding(len(entity_emb_table),100, weights=[entity_emb_table],trainable=True)\n",
    "\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "embedded_sequences=Dropout(0.2)(embedded_sequences)\n",
    "embedded_sequences=Dropout(0.2)(Attention(20,20)([embedded_sequences,embedded_sequences,embedded_sequences]))\n",
    "\n",
    "textrep=Dense(256,activation='relu')(AttLayer()(embedded_sequences))\n",
    "\n",
    "text_encoder = Model([sentence_input], textrep)\n",
    "\n",
    "review_input = Input((MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
    "review_encoders= TimeDistributed(text_encoder)(review_input)\n",
    "\n",
    "review_encoders = Dropout(0.2)(review_encoders)\n",
    "A_input = Input(shape=(MAX_SENTS+len(category)+50,MAX_SENTS+len(category)+50), dtype='float32')\n",
    "\n",
    "all_topic_input = Input((len(category),), dtype='int32')\n",
    "all_entity_input = Input((50,), dtype='int32')\n",
    "\n",
    "all_topic_emb=topic_embedding_layer(all_topic_input)\n",
    "all_entity_emb=Dense(256)(entity_embedding_layer(all_entity_input))\n",
    "xinput=concatenate([review_encoders ,all_topic_emb, all_entity_emb],axis=1)\n",
    "\n",
    "graph_attention_basis = GraphAttention(16,\n",
    "                           attn_heads=16,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([xinput, A_input])\n",
    "\n",
    "\n",
    "graph_pool_n1 = GraphAttention(16,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([graph_attention_basis, A_input])\n",
    "graph_pool_n1 =Activation('softmax')(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(Dense(50)(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(graph_pool_n1))))\n",
    "graph_pool_n1=concatenate([graph_pool_n1,Lambda(lambda x:K.zeros_like(x[:,:,:16]))(all_topic_emb),Lambda(lambda x:K.zeros_like(x[:,:,:16]))(all_entity_emb)],axis=1)\n",
    "\n",
    "\n",
    "graph_pool_t1 = GraphAttention(3,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([graph_attention_basis, A_input])\n",
    "graph_pool_t1 =Activation('softmax')(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(Dense(len(category))(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(graph_pool_t1))))\n",
    "graph_pool_t1=concatenate([Lambda(lambda x:K.zeros_like(x[:,:,:3]))(review_encoders),graph_pool_t1,Lambda(lambda x:K.zeros_like(x[:,:,:3]))(all_entity_emb)],axis=1)\n",
    "\n",
    "\n",
    "graph_pool_e1 = GraphAttention(9,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([graph_attention_basis, A_input])\n",
    "graph_pool_e1 =Activation('softmax')(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(Dense(50)(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(graph_pool_e1))))\n",
    "graph_pool_e1=concatenate([Lambda(lambda x:K.zeros_like(x[:,:,:9]))(review_encoders),Lambda(lambda x:K.zeros_like(x[:,:,:9]))(all_topic_emb),graph_pool_e1],axis=1)\n",
    "\n",
    "\n",
    "Apool_nn1=Dot((2,1))([Permute((2,1))(graph_pool_n1),Dot((1,1))([A_input,graph_pool_n1])])\n",
    "xpool_n1=Dot((1,1))([graph_pool_n1,graph_attention_basis])\n",
    "\n",
    "Apool_tt1=Dot((2,1))([Permute((2,1))(graph_pool_t1),Dot((1,1))([A_input,graph_pool_t1])])\n",
    "xpool_t1=Dot((1,1))([graph_pool_t1,graph_attention_basis])\n",
    "\n",
    "Apool_ee1=Dot((2,1))([Permute((2,1))(graph_pool_e1),Dot((1,1))([A_input,graph_pool_e1])])\n",
    "xpool_e1=Dot((1,1))([graph_pool_e1,graph_attention_basis])\n",
    "\n",
    "Apool_nt1=Dot((2,1))([Permute((2,1))(graph_pool_n1),Dot((1,1))([A_input,graph_pool_t1])])\n",
    "Apool_tn1=Dot((2,1))([Permute((2,1))(graph_pool_t1),Dot((1,1))([A_input,graph_pool_n1])])\n",
    "Apool_ne1=Dot((2,1))([Permute((2,1))(graph_pool_n1),Dot((1,1))([A_input,graph_pool_e1])])\n",
    "Apool_en1=Dot((2,1))([Permute((2,1))(graph_pool_e1),Dot((1,1))([A_input,graph_pool_n1])])\n",
    "Apool_te1=Dot((2,1))([Permute((2,1))(graph_pool_t1),Dot((1,1))([A_input,graph_pool_e1])])\n",
    "Apool_et1=Dot((2,1))([Permute((2,1))(graph_pool_e1),Dot((1,1))([A_input,graph_pool_t1])])\n",
    "Ap1=concatenate([concatenate([Apool_nn1,Apool_tn1,Apool_en1],axis=1),concatenate([Apool_nt1,Apool_tt1,Apool_et1],axis=1),concatenate([Apool_ne1,Apool_te1,Apool_ee1],axis=1)])\n",
    "xp1=concatenate([xpool_n1,xpool_t1,xpool_e1],axis=1)\n",
    "\n",
    "graph_pool_n2 = GraphAttention(1,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([xp1, Ap1])\n",
    "graph_pool_n2 =Activation('softmax')(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(Dense(16)(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(graph_pool_n2))))\n",
    "graph_pool_n2=concatenate([graph_pool_n2,Lambda(lambda x:K.zeros_like(x[:,:,:1]))(xpool_t1),Lambda(lambda x:K.zeros_like(x[:,:,:1]))(xpool_e1)],axis=1)\n",
    "\n",
    "graph_pool_t2 = GraphAttention(1,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([xp1, Ap1])\n",
    "graph_pool_t2 =Activation('softmax')(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(Dense(3)(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(graph_pool_t2))))\n",
    "graph_pool_t2=concatenate([Lambda(lambda x:K.zeros_like(x[:,:,:1]))(xpool_n1),graph_pool_t2,Lambda(lambda x:K.zeros_like(x[:,:,:1]))(xpool_e1)],axis=1)\n",
    "\n",
    "graph_pool_e2 = GraphAttention(1,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([xp1, Ap1])\n",
    "graph_pool_e2 =Activation('softmax')(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(Dense(9)(Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(graph_pool_e2))))\n",
    "graph_pool_e2=concatenate([Lambda(lambda x:K.zeros_like(x[:,:,:1]))(xpool_n1),Lambda(lambda x:K.zeros_like(x[:,:,:1]))(xpool_t1),graph_pool_e2],axis=1)\n",
    "\n",
    "\n",
    "Apool_nn2=Dot((2,1))([Permute((2,1))(graph_pool_n2),Dot((1,1))([Ap1,graph_pool_n2])])\n",
    "xpool_n2=Dot((1,1))([graph_pool_n2,xp1])\n",
    "\n",
    "Apool_tt2=Dot((2,1))([Permute((2,1))(graph_pool_t2),Dot((1,1))([Ap1,graph_pool_t2])])\n",
    "xpool_t2=Dot((1,1))([graph_pool_t2,xp1])\n",
    "\n",
    "Apool_ee2=Dot((2,1))([Permute((2,1))(graph_pool_e2),Dot((1,1))([Ap1,graph_pool_e2])])\n",
    "xpool_e2=Dot((1,1))([graph_pool_e2,xp1])\n",
    "\n",
    "Apool_nt2=Dot((2,1))([Permute((2,1))(graph_pool_n2),Dot((1,1))([Ap1,graph_pool_t2])])\n",
    "Apool_tn2=Dot((2,1))([Permute((2,1))(graph_pool_t2),Dot((1,1))([Ap1,graph_pool_n2])])\n",
    "Apool_ne2=Dot((2,1))([Permute((2,1))(graph_pool_n2),Dot((1,1))([Ap1,graph_pool_e2])])\n",
    "Apool_en2=Dot((2,1))([Permute((2,1))(graph_pool_e2),Dot((1,1))([Ap1,graph_pool_n2])])\n",
    "Apool_te2=Dot((2,1))([Permute((2,1))(graph_pool_t2),Dot((1,1))([Ap1,graph_pool_e2])])\n",
    "Apool_et2=Dot((2,1))([Permute((2,1))(graph_pool_e2),Dot((1,1))([Ap1,graph_pool_t2])])\n",
    "Ap2=concatenate([concatenate([Apool_nn2,Apool_tn2,Apool_en2],axis=1),concatenate([Apool_nt2,Apool_tt2,Apool_et2],axis=1),concatenate([Apool_ne2,Apool_te2,Apool_ee2],axis=1)])\n",
    "xp2=concatenate([xpool_n2,xpool_t2,xpool_e2],axis=1)\n",
    "\n",
    "diff_pool = GraphAttention(1,\n",
    "                           attn_heads=1,\n",
    "                           attn_heads_reduction='concat',\n",
    "                           dropout_rate=0.2,\n",
    "                           activation='elu')([xp2, Ap2])\n",
    "\n",
    "diff_pool =Flatten()(Activation('softmax')(diff_pool))\n",
    "u_att=keras.layers.Dot((1, 1))([xp2, diff_pool])\n",
    "\n",
    "\n",
    "candidates=Input((1+npratio,MAX_SENT_LENGTH,), dtype='int32')\n",
    "candidate_vecs=TimeDistributed(text_encoder)(candidates)\n",
    "\n",
    "candidatestopic=Input((1+npratio,), dtype='int32')\n",
    "candidatesentity=Input((1+npratio,5), dtype='int32')\n",
    "candidate_topicemb=topic_embedding_layer(candidatestopic)\n",
    "entity_dim_dense=Dense(256)\n",
    "entity_att=AttLayer()\n",
    "candidate_entityemb = TimeDistributed(entity_att)(TimeDistributed(TimeDistributed(entity_dim_dense))(entity_embedding_layer(candidatesentity)))\n",
    "\n",
    "view_emb=concatenate([Lambda(lambda y:K.expand_dims(y,axis=2))(x) for x in [candidate_vecs,candidate_topicemb,candidate_entityemb]],axis=2)\n",
    "\n",
    "view_att=AttLayer()\n",
    "\n",
    "candidate_emb=TimeDistributed(view_att)(view_emb)\n",
    "\n",
    "logits = Lambda(lambda x:K.clip(x,-5,5))(keras.layers.dot([u_att, candidate_emb], axes=-1))\n",
    "logits = keras.layers.Activation(keras.activations.softmax)(logits)\n",
    "\n",
    "\n",
    "model = Model([candidates,review_input,candidatestopic,candidatesentity,all_entity_input,all_topic_input ,A_input], [logits])\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer=Adam(lr=0.0001), metrics=['acc'])\n",
    "\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_topic_input = Input(shape=(1,), dtype='int32')\n",
    "candidate_one_entity_input = Input(shape=(5,), dtype='int32')\n",
    "candidate_one_vec = text_encoder([candidate_one])\n",
    "candidate_one_topicemb=topic_embedding_layer(candidate_one_topic_input)\n",
    "candidate_one_entityemb=entity_att(TimeDistributed(entity_dim_dense)(entity_embedding_layer(candidate_one_entity_input)))\n",
    "\n",
    "candidate_one_view_emb=concatenate([Lambda(lambda y:K.expand_dims(y,axis=1))(x) for x in [candidate_one_vec,candidate_one_entityemb]]+[candidate_one_topicemb],axis=1)\n",
    "\n",
    "candidate_one_vec = view_att(candidate_one_view_emb)\n",
    "candidate_encoder = keras.Model([candidate_one,candidate_one_topic_input,candidate_one_entity_input], candidate_one_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_user(batch_size):\n",
    "    idlist = np.arange(len(test_user_his))  \n",
    "    batches = [idlist[range(batch_size*i, min(len(test_user_his), batch_size*(i+1)))] for i in range(len(test_user_his)//batch_size+1)]\n",
    "    while (True):\n",
    "        for i in batches: \n",
    "\n",
    "            user_his=news_words[test_user_his[i]]\n",
    "            user_topic=news_topic[test_user_his[i]]\n",
    "            user_entity=news_entity[test_user_his[i]]\n",
    "            user_entity_feature=[]\n",
    "            all_A=[]\n",
    "            for s in range(len(i)):\n",
    "                Asize=len(test_user_his[i][s])+len(category)+50\n",
    "                newsA=np.zeros((Asize,Asize))\n",
    "                entityid_set={}\n",
    "                entityid_set_ids=[]\n",
    "                for el in range(len(user_entity[s])):\n",
    "                    for e in range(len(user_entity[s][el])):\n",
    "                        if user_entity[s][el][e] not in entityid_set:\n",
    "                            entityid_set[user_entity[s][el][e]]=len(entityid_set)\n",
    "                            entityid_set_ids.append(user_entity[s][el][e])\n",
    "                        if user_entity[s][el][e]!=0  and entityid_set[user_entity[s][el][e]]<50:\n",
    "                            newsA[el][len(test_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]]=1\n",
    "                            newsA[len(test_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]][el]=1\n",
    "                            newsA[len(test_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]][len(test_user_his[i][s])+len(category)+entityid_set[user_entity[s][el][e]]]=1\n",
    "                entityid_set_ids=entityid_set_ids[:50]\n",
    "                entityid_set_ids+=[0]*(50-len(entityid_set_ids))\n",
    "        \n",
    "                for m in range(len(test_user_his[i][s])):\n",
    "                    if test_user_his[i][s][m]!=0:\n",
    "                        newsA[m][m]=1\n",
    "                        if m>=1 and test_user_his[i][s][m-1]!=0:\n",
    "                            newsA[m][m-1]=1\n",
    "                        if m<len(test_user_his[i][s])-1 and test_user_his[i][s][m+1]!=0:\n",
    "                            newsA[m][m+1]=1\n",
    "                    if user_topic[s][m]!=0:\n",
    "                        newsA[m][len(test_user_his[i][s])+user_topic[s][m]]=1\n",
    "                        newsA[len(test_user_his[i][s])+user_topic[s][m]][m]=1\n",
    "                        newsA[len(test_user_his[i][s])+user_topic[s][m]][len(test_user_his[i][s])+utopic[s][m]]=1\n",
    "                user_entity_feature.append(entityid_set_ids)\n",
    "                all_A.append(newsA)\n",
    "            user_entity_feature=np.array(user_entity_feature)                \n",
    "    \n",
    "    \n",
    "            all_A=np.array(all_A,dtype='float32')\n",
    "            \n",
    "            topics=np.array([list(range(len(category)))]*len(i),dtype='int32')\n",
    "            \n",
    "            yield ([user_his,user_entity_feature,topics,all_A])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for ep in range(3):\n",
    "\n",
    "    traingen=generate_batch_data_random(30)\n",
    "    \n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(train_label)//30)\n",
    "    usermodel = Model([review_input,all_entity_input,all_topic_input ,A_input], u_att)\n",
    "\n",
    "    valgen=generate_batch_data_user(30) \n",
    "    uservec=usermodel.predict_generator(valgen,steps=len(test_user_his)//30+1,verbose=1)\n",
    "    \n",
    "    newsvec=candidate_encoder.predict([news_words,news_topic,news_entity],batch_size=50,verbose=1)\n",
    "    \n",
    "    predictsession=[]\n",
    "    \n",
    "    for i in range(len(uservec)):\n",
    "        newscand=newsvec[test_candidate[test_index[i][0]:test_index[i][1]]]\n",
    "        uvector=uservec[i]\n",
    "        scores=1/(1+np.exp(-np.tensordot(uvector,newscand,axes=(0,1))))\n",
    "        predictsession.append(scores)\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    \n",
    "    for t in range(len(predictsession)):\n",
    "        m=test_index[t]\n",
    "        if m[1]-m[0]>1:\n",
    "            all_auc.append(roc_auc_score(test_label[m[0]:m[1]],predictsession[t]))\n",
    "            all_mrr.append(mrr_score(test_label[m[0]:m[1]],predictsession[t]))\n",
    "            all_ndcg.append(ndcg_score(test_label[m[0]:m[1]],predictsession[t],k=5))\n",
    "            all_ndcg2.append(ndcg_score(test_label[m[0]:m[1]],predictsession[t],k=10))\n",
    "            if len(all_auc)%10000==0:\n",
    "                print(len(all_auc))\n",
    "    results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "    print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
